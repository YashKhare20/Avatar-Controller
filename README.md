# Extended-Reality Remote Assistance for Manufacturing with LLM-driven Avatars

## Overview

This project leverages Extended Reality (XR) and Large Language Models (LLMs) to develop hyper-personalized, context-aware remote assistance avatars for manufacturing environments. The system is designed to significantly reduce manufacturing downtime and enhance operational efficiency by providing precise and responsive troubleshooting for industrial equipment.

### Key Features
- **Context-Aware Avatars**: Avatars are equipped with domain-specific knowledge, enabling accurate guidance for equipment troubleshooting.
- **Integration with XR**: Immersive extended reality environment for intuitive interaction between users and virtual avatars.
- **Speech and Text Processing**: Real-time speech-to-text, chat completion, and text-to-speech functionalities for seamless communication.
- **Performance Metrics**:
  - 35% reduction in manufacturing downtime.
  - 92% accuracy in resolving complex industrial equipment malfunctions.

![Avatar Interaction Example](file-bSFV1HBooDI7bAHY69SyYRp6)

*Figure 1: Interaction between the user and the LLM-driven avatar in an XR environment.*

---

## Technologies Used
- **Unity3D**: Core development platform for creating the XR environment.
- **C#**: Scripting language for implementing features and API integrations in Unity.
- **Large Language Models (LLMs)**: For intelligent chat-based troubleshooting.
- **LangChain**: Framework for domain-specific prompt engineering.
- **APIs**:
  - **Speech-to-Text**: Converts verbal queries into text for LLM processing.
  - **Chat Completion**: LLM-driven context-aware responses.
  - **Text-to-Speech**: Converts responses back to audio for intuitive user interaction.

---

## Features and Implementation

### 1. Hyper-Personalized Avatars
- **Domain-Specific Prompt Engineering**: Avatars are tailored to manufacturing use cases using LangChain to fine-tune interactions.
- **Adaptability**: Avatars provide context-sensitive assistance, making them highly effective for troubleshooting diverse industrial scenarios.

### 2. Real-Time Interaction
- **Speech-to-Text Integration**: Enables users to communicate naturally with the avatars through voice.
- **Chat Completion API**: Processes input queries and generates precise, actionable insights.
- **Text-to-Speech Integration**: Avatars deliver responses in an intuitive auditory format.

![Speech-to-Text Pipeline](file-gbcIpK6cN0l6ZAlFpHa5XcU1)

*Figure 2: Workflow showing the speech-to-text and chat completion pipeline for seamless user-avatar interaction.*

### 3. High-Accuracy Troubleshooting
- Reduces miscommunication and troubleshooting errors with a 92% success rate in resolving complex equipment malfunctions.

---

## Project Outcomes
- **Operational Efficiency**: Achieved a 35% reduction in manufacturing downtime, translating to substantial cost savings and increased productivity.
- **Enhanced User Experience**: Delivered a seamless and immersive interaction model for manufacturing staff.
- **Scalability**: The system is designed to adapt to various manufacturing settings and integrate with additional tools and APIs.

---

## How to Use

### Prerequisites
- **Hardware**: XR-enabled device (e.g., HoloLens, VR headset).
- **Software**:
  - Unity3D installed with necessary dependencies.
  - Access to LLM APIs (e.g., OpenAI, LangChain).
  - API keys for speech-to-text, chat completion, and text-to-speech services.

### Setup Instructions
1. Clone the repository:
   ```bash
   git clone https://github.com/YashKhare20/Avatar-Controller/
   ```
2. Open the project in Unity3D.
3. Configure API keys in the project settings file.
4. Deploy to your XR-enabled device or simulator.

### Running the System
- Launch the XR environment.
- Interact with the avatar using voice commands.
- Receive real-time guidance and troubleshooting support.

---

## Future Enhancements
- Expanding avatar capabilities to cover more manufacturing domains.
- Integration with IoT sensors for enhanced situational awareness.
- Multilingual support for global manufacturing environments.

---

## Contributors
- **Yash Khare** 
- **Mohan Bhosale**
- **Page Patterson**

---

## License
This project is licensed under the [MIT License](LICENSE).

Feel free to contribute or raise issues in the [repository](https://github.com/your-repo/Extended-Reality-Remote-Assistance). ðŸš€
